{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daed8f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from utils.helper import load_multiple_data\n",
    "from utils.config import get_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79df3e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_positive_pairs_for_contrastive_learning(\n",
    "    data: List[Dict[str, str]],\n",
    "    include_identity_pairs: bool = False\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    주어진 데이터 형식에서 Contrastive Learning을 위한 긍정 쌍(Positive Pair)을 생성합니다.\n",
    "\n",
    "    각 의미 그룹 내 4가지 버전의 문장들 사이에서 가능한 모든 순서 있는 쌍을 생성하여,\n",
    "    모든 버전이 서로를 긍정적인 예시로 끌어당기도록 학습 데이터를 구성합니다.\n",
    "    (예: MultipleNegativesRankingLoss와 함께 사용하기 적합)\n",
    "\n",
    "    Args:\n",
    "        data: 각 요소가 {'EtoK': ..., 'KtoE': ..., 'English': ..., 'Korean': ...}\n",
    "              형식의 딕셔너리인 리스트. 각 딕셔너리는 같은 의미를 가진 문장 그룹임.\n",
    "        include_identity_pairs: 동일한 문장으로 구성된 쌍 (예: ('text_a', 'text_a'))을\n",
    "                                 포함할지 여부. 일반적으로 Contrastive Loss에서는 False.\n",
    "\n",
    "    Returns:\n",
    "        긍정적인 (anchor, positive) 문장 쌍의 리스트.\n",
    "        예: [('EtoK_sent1', 'KtoE_sent1'), ('EtoK_sent1', 'English_sent1'), ...]\n",
    "    \"\"\"\n",
    "    training_pairs: List[Tuple[str, str]] = []\n",
    "    sentence_keys: List[str] = ['EtoK', 'KtoE', 'English', 'Korean']\n",
    "\n",
    "    print(f\"원본 데이터 그룹 수: {len(data)}\")\n",
    "\n",
    "    for item in data:\n",
    "        # 현재 의미 그룹의 4가지 버전 문장 추출\n",
    "        sentences_in_group: List[str] = [item[key] for key in sentence_keys if key in item and item[key]]\n",
    "\n",
    "        # 유효한 문장이 2개 미만인 경우 건너뛰기 (쌍을 만들 수 없음)\n",
    "        if len(sentences_in_group) < 2:\n",
    "            print(f\"Warning: 유효한 문장이 2개 미만인 그룹 발견. 건너<0xEB><0x8B>니다: {item}\")\n",
    "            continue\n",
    "\n",
    "        # 그룹 내에서 모든 순서 있는 쌍 생성\n",
    "        for i in range(len(sentences_in_group)):\n",
    "            for j in range(len(sentences_in_group)):\n",
    "                if i == j and not include_identity_pairs:\n",
    "                    continue # 자기 자신과의 쌍은 제외 (옵션)\n",
    "\n",
    "                anchor = sentences_in_group[i]\n",
    "                positive = sentences_in_group[j]\n",
    "\n",
    "                # 문장이 비어있지 않은 경우에만 쌍 추가\n",
    "                if anchor and positive:\n",
    "                    training_pairs.append((anchor, positive))\n",
    "\n",
    "    print(f\"생성된 긍정 쌍(Positive Pair) 수: {len(training_pairs)}\")\n",
    "    if len(data) > 0:\n",
    "         print(f\"예상 쌍 수 (그룹당 최대 {len(sentence_keys)*(len(sentence_keys)-1) if not include_identity_pairs else len(sentence_keys)**2}): 약 {len(data) * len(sentence_keys)*(len(sentence_keys)-1 if not include_identity_pairs else len(sentence_keys))}\")\n",
    "\n",
    "    return training_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89a54019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 4.1.0, however, your version is 3.4.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 디렉토리: dataset/\n",
      "로드할 파일: dataset/code-switch.json\n",
      "로드된 데이터 일부 (첫 번째 항목): {'EtoK': 'Could you explain how 양자 컴퓨팅 and 양자 얽힘 make it possible to solve certain problems faster than classical methods?', 'KtoE': '고전적 방식보다 더 효율적인 quantum computing과 quantum entanglement의 핵심 원리는 무엇인가요?', 'English': 'Could you explain how quantum computing and quantum entanglement make it possible to solve certain problems faster than classical methods?', 'Korean': '양자 컴퓨팅과 양자 얽힘이 일부 문제를 고전적 방법보다 더 빠르게 해결하도록 만드는 핵심 원리는 무엇인가요?'}\n",
      "로드할 파일: dataset/code-switch1.json\n",
      "로드된 데이터 일부 (첫 번째 항목): {'topic': 'science/technology', 'type': 'fact', 'complexity': 'simple', 'contents': 'brief', 'EtoK': 'The 원자 is the basic unit of matter.', 'KtoE': '원자는 matter의 기본 단위이다.', 'English': 'The atom is the basic unit of matter.', 'Korean': '원자는 물질의 기본 단위이다.'}\n",
      "로드할 파일: dataset/code-switch2.json\n",
      "로드된 데이터 일부 (첫 번째 항목): {'topic': 'science/technology', 'type': 'fact', 'complexity': 'simple', 'contents': 'brief', 'EtoK': 'The 블랙홀 is a space region with extreme gravity.', 'KtoE': '블랙홀은 극심한 gravity를 가진 우주 영역이다.', 'English': 'The black hole is a space region with extreme gravity.', 'Korean': '블랙홀은 극심한 중력을 가진 우주 영역이다.'}\n",
      "총 로드된 데이터 항목 수: 269\n",
      "원본 데이터 그룹 수: 269\n",
      "생성된 긍정 쌍(Positive Pair) 수: 3228\n",
      "예상 쌍 수 (그룹당 최대 12): 약 3228\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. 모델 로드\n",
    "model_name = \"hyoseok1989/bge-m3-finetuned\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# 2. 데이터 준비 (이전 단계에서 생성한 함수 사용)\n",
    "data_dir = \"dataset/\"\n",
    "if isinstance(data_dir, str):\n",
    "    data = load_multiple_data(data_dir)\n",
    "else:\n",
    "    data = load_multiple_data(data_dir)\n",
    "# positive_pairs: List[Tuple[str, str]]\n",
    "positive_pairs = create_positive_pairs_for_contrastive_learning(data)\n",
    "train_examples = [InputExample(texts=[pair[0], pair[1]]) for pair in positive_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f39fa6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DataLoader 준비\n",
    "batch_size = 16 # GPU 메모리에 맞게 조정\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# 4. 손실 함수 정의 (라이브러리 내장 함수 사용)\n",
    "# MultipleNegativesRankingLoss는 InputExample(texts=[anchor, positive]) 형태를 기대하며,\n",
    "# 배치 내 다른 모든 샘플을 자동으로 네거티브로 사용합니다.\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67795b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m warmup_steps = math.ceil(\u001b[38;5;28mlen\u001b[39m(train_dataloader) * num_epochs * \u001b[32m0.1\u001b[39m) \u001b[38;5;66;03m# 예: 10% 웜업\u001b[39;00m\n\u001b[32m      4\u001b[39m output_save_path = \u001b[33m'\u001b[39m\u001b[33m./bge-m3-codeswitch-finetuned\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_objectives\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m          \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m          \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m          \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m          \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_save_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-checkpoints\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 체크포인트 저장 경로\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m          \u001b[49m\u001b[43mcheckpoint_save_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 예: 에폭당 2번 저장\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFine-tuning 완료. 모델 저장 경로: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/sentence_transformers/fit_mixin.py:385\u001b[39m, in \u001b[36mFitMixin.fit\u001b[39m\u001b[34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    383\u001b[39m     trainer.add_callback(SaveModelCallback(output_path, evaluator, save_best_model))\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/transformers/trainer.py:2556\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2549\u001b[39m context = (\n\u001b[32m   2550\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2551\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2552\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2553\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2554\u001b[39m )\n\u001b[32m   2555\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2556\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2559\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2561\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2562\u001b[39m ):\n\u001b[32m   2563\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2564\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/transformers/trainer.py:3764\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3761\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3762\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3764\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/accelerate/accelerator.py:2359\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2357\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 5. 모델 Fine-tuning 실행\n",
    "num_epochs = 1\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) # 예: 10% 웜업\n",
    "output_save_path = './bge-m3-codeswitch-finetuned'\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=num_epochs,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=output_save_path,\n",
    "          show_progress_bar=True,\n",
    "          checkpoint_path=output_save_path + '-checkpoints', # 체크포인트 저장 경로\n",
    "          checkpoint_save_steps=len(train_dataloader) // 2 # 예: 에폭당 2번 저장\n",
    "         )\n",
    "\n",
    "print(f\"Fine-tuning 완료. 모델 저장 경로: {output_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bge-m3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
