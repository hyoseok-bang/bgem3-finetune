{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/bgem3-finetune/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualDataset(Dataset):\n",
    "    def __init__(self, data_list, tokenizer, max_length=128):\n",
    "        self.data = data_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize all versions of the sentence\n",
    "        english = self.tokenizer(item[\"English\"], padding=\"max_length\", \n",
    "                                truncation=True, max_length=self.max_length, \n",
    "                                return_tensors=\"pt\")\n",
    "        \n",
    "        etok = self.tokenizer(item[\"EtoK\"], padding=\"max_length\", \n",
    "                             truncation=True, max_length=self.max_length, \n",
    "                             return_tensors=\"pt\")\n",
    "        \n",
    "        ktoe = self.tokenizer(item[\"KtoE\"], padding=\"max_length\", \n",
    "                             truncation=True, max_length=self.max_length, \n",
    "                             return_tensors=\"pt\")\n",
    "        \n",
    "        korean = self.tokenizer(item[\"Korean\"], padding=\"max_length\", \n",
    "                               truncation=True, max_length=self.max_length, \n",
    "                               return_tensors=\"pt\")\n",
    "        \n",
    "        # Return a flat dictionary with prefixed keys\n",
    "        return {\n",
    "            \"english_input_ids\": english[\"input_ids\"].squeeze(0),\n",
    "            \"english_attention_mask\": english[\"attention_mask\"].squeeze(0),\n",
    "            \"etok_input_ids\": etok[\"input_ids\"].squeeze(0),\n",
    "            \"etok_attention_mask\": etok[\"attention_mask\"].squeeze(0),\n",
    "            \"ktoe_input_ids\": ktoe[\"input_ids\"].squeeze(0),\n",
    "            \"ktoe_attention_mask\": ktoe[\"attention_mask\"].squeeze(0),\n",
    "            \"korean_input_ids\": korean[\"input_ids\"].squeeze(0),\n",
    "            \"korean_attention_mask\": korean[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "# --- Helper Function to Calculate Pairwise Loss ---\n",
    "def calculate_pairwise_loss(\n",
    "    emb_k: torch.Tensor,\n",
    "    emb_e: torch.Tensor,\n",
    "    emb_ktoe: torch.Tensor,\n",
    "    emb_etok: torch.Tensor,\n",
    "    loss_type: str = 'cosine' # 'cosine' or 'mse'\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the sum of pairwise losses (1-cosine_sim or mse)\n",
    "    between all unique pairs in the quadruplet.\n",
    "\n",
    "    Args:\n",
    "        emb_k, emb_e, emb_ktoe, emb_etok: Tensors of shape (batch_size, embedding_dim)\n",
    "        loss_type: 'cosine' for 1 - cosine_similarity, 'mse' for squared L2 distance.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size,) containing the sum of pairwise losses for each item.\n",
    "    \"\"\"\n",
    "    embeddings = [emb_k, emb_e, emb_ktoe, emb_etok]\n",
    "    batch_size = emb_k.shape[0]\n",
    "    total_pairwise_loss = torch.zeros(batch_size, device=emb_k.device)\n",
    "    num_pairs = 0\n",
    "\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i + 1, len(embeddings)):\n",
    "            emb_i = embeddings[i]\n",
    "            emb_j = embeddings[j]\n",
    "\n",
    "            if loss_type == 'cosine':\n",
    "                # 1 - cosine similarity\n",
    "                sim = F.cosine_similarity(emb_i, emb_j, dim=1)\n",
    "                loss = 1.0 - sim\n",
    "            elif loss_type == 'mse':\n",
    "                # Squared L2 distance\n",
    "                loss = torch.sum((emb_i - emb_j) ** 2, dim=1)\n",
    "            else:\n",
    "                raise ValueError(\"loss_type must be 'cosine' or 'mse'\")\n",
    "\n",
    "            total_pairwise_loss += loss\n",
    "            num_pairs += 1\n",
    "\n",
    "    # Optional: Average over the number of pairs if desired,\n",
    "    # but summing works fine as it's consistent.\n",
    "    # return total_pairwise_loss / num_pairs\n",
    "    return total_pairwise_loss\n",
    "\n",
    "# --- Helper Function for Scale Regularization ---\n",
    "def calculate_scale_regularization(\n",
    "    emb_k: torch.Tensor,\n",
    "    emb_e: torch.Tensor,\n",
    "    emb_ktoe: torch.Tensor,\n",
    "    emb_etok: torch.Tensor,\n",
    "    target_norm: float\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the scale regularization loss.\n",
    "\n",
    "    Args:\n",
    "        emb_k, emb_e, emb_ktoe, emb_etok: Tensors of shape (batch_size, embedding_dim)\n",
    "        target_norm: The target L2 norm scalar value.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size,) containing the scale regularization loss for each item.\n",
    "    \"\"\"\n",
    "    embeddings = [emb_k, emb_e, emb_ktoe, emb_etok]\n",
    "    total_scale_loss = torch.zeros(emb_k.shape[0], device=emb_k.device)\n",
    "\n",
    "    for emb in embeddings:\n",
    "        norm = torch.norm(emb, p=2, dim=1)\n",
    "        scale_loss = (norm - target_norm) ** 2\n",
    "        total_scale_loss += scale_loss\n",
    "\n",
    "    # Average over the 4 embedding types\n",
    "    return total_scale_loss / len(embeddings)\n",
    "\n",
    "\n",
    "# --- Loss Function Implementations ---\n",
    "\n",
    "def compute_cosine_loss_with_scale_reg(\n",
    "    emb_k: torch.Tensor,\n",
    "    emb_e: torch.Tensor,\n",
    "    emb_ktoe: torch.Tensor,\n",
    "    emb_etok: torch.Tensor,\n",
    "    target_norm: float,\n",
    "    lambda_scale: float = 0.01 # Hyperparameter to balance terms\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes Loss = (Pairwise 1-CosineSim) + lambda * (Scale Regularization)\n",
    "\n",
    "    Args:\n",
    "        emb_k, emb_e, emb_ktoe, emb_etok: Tensors of shape (batch_size, embedding_dim)\n",
    "        target_norm: The target L2 norm.\n",
    "        lambda_scale: Weight for the scale regularization term.\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss tensor averaged over the batch.\n",
    "    \"\"\"\n",
    "    # Pairwise Cosine Similarity Loss (mean over batch)\n",
    "    pairwise_loss = calculate_pairwise_loss(emb_k, emb_e, emb_ktoe, emb_etok, loss_type='cosine')\n",
    "    loss_sim = torch.mean(pairwise_loss)\n",
    "\n",
    "    # Scale Regularization Loss (mean over batch)\n",
    "    scale_loss = calculate_scale_regularization(emb_k, emb_e, emb_ktoe, emb_etok, target_norm)\n",
    "    loss_scale = torch.mean(scale_loss)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_sim + lambda_scale * loss_scale\n",
    "    return total_loss\n",
    "\n",
    "def compute_mse_loss_with_scale_reg(\n",
    "    emb_k: torch.Tensor,\n",
    "    emb_e: torch.Tensor,\n",
    "    emb_ktoe: torch.Tensor,\n",
    "    emb_etok: torch.Tensor,\n",
    "    target_norm: float,\n",
    "    lambda_scale: float = 0.01 # Hyperparameter to balance terms\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes Loss = (Pairwise MSE) + lambda * (Scale Regularization)\n",
    "\n",
    "    Args:\n",
    "        emb_k, emb_e, emb_ktoe, emb_etok: Tensors of shape (batch_size, embedding_dim)\n",
    "        target_norm: The target L2 norm.\n",
    "        lambda_scale: Weight for the scale regularization term.\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss tensor averaged over the batch.\n",
    "    \"\"\"\n",
    "    # Pairwise MSE Loss (mean over batch)\n",
    "    pairwise_loss = calculate_pairwise_loss(emb_k, emb_e, emb_ktoe, emb_etok, loss_type='mse')\n",
    "    loss_dist = torch.mean(pairwise_loss)\n",
    "\n",
    "    # Scale Regularization Loss (mean over batch)\n",
    "    scale_loss = calculate_scale_regularization(emb_k, emb_e, emb_ktoe, emb_etok, target_norm)\n",
    "    loss_scale = torch.mean(scale_loss)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_dist + lambda_scale * loss_scale\n",
    "    return total_loss\n",
    "\n",
    "# --- Placeholder for Scale-Regularized Contrastive Loss (InfoNCE style) ---\n",
    "# This requires a more complex setup with negative sampling, often done via in-batch negatives.\n",
    "\n",
    "def compute_infonce_loss_with_scale_reg(\n",
    "    emb_k: torch.Tensor,\n",
    "    emb_e: torch.Tensor,\n",
    "    emb_ktoe: torch.Tensor,\n",
    "    emb_etok: torch.Tensor,\n",
    "    target_norm: float,\n",
    "    lambda_scale: float = 0.01,\n",
    "    temperature: float = 0.07\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes InfoNCE-style Contrastive Loss + Scale Regularization.\n",
    "    Assumes in-batch negative sampling.\n",
    "\n",
    "    Args:\n",
    "        emb_k, emb_e, emb_ktoe, emb_etok: Tensors of shape (batch_size, embedding_dim)\n",
    "        target_norm: The target L2 norm.\n",
    "        lambda_scale: Weight for the scale regularization term.\n",
    "        temperature: Temperature scaling factor for InfoNCE.\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss tensor averaged over the batch.\n",
    "    \"\"\"\n",
    "    device = emb_k.device\n",
    "    batch_size = emb_k.shape[0]\n",
    "    embedding_dim = emb_k.shape[1]\n",
    "\n",
    "    # Concatenate all embeddings for easier similarity matrix calculation\n",
    "    # Shape: (batch_size * 4, embedding_dim)\n",
    "    all_embeddings = torch.cat([emb_k, emb_e, emb_ktoe, emb_etok], dim=0)\n",
    "\n",
    "    # Calculate pairwise cosine similarities (logits)\n",
    "    # Shape: (batch_size * 4, batch_size * 4)\n",
    "    logits = F.cosine_similarity(all_embeddings.unsqueeze(1), all_embeddings.unsqueeze(0), dim=2)\n",
    "    logits /= temperature # Apply temperature scaling\n",
    "\n",
    "    # Create labels and masks for InfoNCE\n",
    "    # Labels indicate the index of the positive counterpart within the large batch.\n",
    "    # For InfoNCE, typically anchor vs one positive, others negative.\n",
    "    # Here, we have multiple positives per anchor within the quadruplet.\n",
    "\n",
    "    # Simplified approach: Treat each K as anchor, E/KtoE/EtoK as positives. Repeat for other anchors.\n",
    "    total_contrastive_loss = torch.tensor(0.0, device=device)\n",
    "    anchors = [emb_k, emb_e, emb_ktoe, emb_etok]\n",
    "    anchor_indices = [torch.arange(batch_size) + i * batch_size for i in range(4)] # Indices for K, E, KtoE, EtoK in all_embeddings\n",
    "\n",
    "    for i in range(4): # Iterate through K, E, KtoE, EtoK as anchors\n",
    "        current_anchor_indices = anchor_indices[i]\n",
    "        # Logits for the current anchors vs all embeddings\n",
    "        anchor_logits = logits[current_anchor_indices] # Shape: (batch_size, batch_size * 4)\n",
    "\n",
    "        # Create target labels: indicate positive pairs\n",
    "        # Positive keys are the other embeddings from the *same* original item.\n",
    "        labels = torch.zeros_like(anchor_logits, dtype=torch.bool)\n",
    "        for j in range(4):\n",
    "            if i == j: continue # Don't mark anchor as its own positive\n",
    "            positive_indices = anchor_indices[j]\n",
    "            # Mark True where the column index corresponds to a positive pair for the row anchor\n",
    "            # Need element-wise check. A bit tricky with broadcasted indices.\n",
    "            # Let's usearange for batch items:\n",
    "            batch_indices = torch.arange(batch_size, device=device)\n",
    "            labels[batch_indices, positive_indices] = True\n",
    "\n",
    "        # InfoNCE Loss calculation: log_softmax over all pairs, gather positives.\n",
    "        log_prob = F.log_softmax(anchor_logits, dim=1)\n",
    "\n",
    "        # Sum log probabilities of *all* positive pairs for the anchor\n",
    "        # Avoid division by zero if no positives (shouldn't happen here)\n",
    "        mean_log_prob_pos = (log_prob * labels).sum(1) / labels.sum(1).clamp(min=1)\n",
    "\n",
    "        # Contrastive loss is the negative of the mean log probability of positives\n",
    "        contrastive_loss = -mean_log_prob_pos\n",
    "        total_contrastive_loss += contrastive_loss.mean() # Average over batch\n",
    "\n",
    "    # Average contrastive loss over the 4 anchor types\n",
    "    loss_contrastive = total_contrastive_loss / 4.0\n",
    "\n",
    "    # Scale Regularization Loss (mean over batch)\n",
    "    scale_loss = calculate_scale_regularization(emb_k, emb_e, emb_ktoe, emb_etok, target_norm)\n",
    "    loss_scale = torch.mean(scale_loss)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_contrastive + lambda_scale * loss_scale\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 기존 Helper Functions (calculate_scale_regularization)은 그대로 사용 ---\n",
    "# (위에 제공된 코드에 이미 포함되어 있다고 가정)\n",
    "\n",
    "def calculate_scale_regularization(\n",
    "    emb_k: torch.Tensor,\n",
    "    emb_e: torch.Tensor,\n",
    "    emb_ktoe: torch.Tensor,\n",
    "    emb_etok: torch.Tensor,\n",
    "    target_norm: float\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the scale regularization loss. (Identical to previous version)\n",
    "\n",
    "    Args:\n",
    "        emb_k, emb_e, emb_ktoe, emb_etok: Tensors of shape (batch_size, embedding_dim)\n",
    "        target_norm: The target L2 norm scalar value.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size,) containing the scale regularization loss for each item.\n",
    "    \"\"\"\n",
    "    embeddings = [emb_k, emb_e, emb_ktoe, emb_etok]\n",
    "    total_scale_loss = torch.zeros(emb_k.shape[0], device=emb_k.device)\n",
    "\n",
    "    for emb in embeddings:\n",
    "        norm = torch.norm(emb, p=2, dim=1)\n",
    "        scale_loss = (norm - target_norm) ** 2\n",
    "        total_scale_loss += scale_loss\n",
    "\n",
    "    # Average over the 4 embedding types\n",
    "    return total_scale_loss / len(embeddings)\n",
    "\n",
    "# --- New Loss Function with English Anchor ---\n",
    "\n",
    "def compute_cosine_loss_anchor_e_with_scale_reg(\n",
    "    emb_k: torch.Tensor,\n",
    "    emb_e: torch.Tensor,\n",
    "    emb_ktoe: torch.Tensor,\n",
    "    emb_etok: torch.Tensor,\n",
    "    target_norm: float,\n",
    "    lambda_scale: float = 0.01 # Hyperparameter to balance terms\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes Loss where K, KtoE, EtoK embeddings are pulled towards a\n",
    "    fixed English anchor (E), plus scale regularization on all embeddings.\n",
    "\n",
    "    Loss = (1-cos(K, E_detach)) + (1-cos(KtoE, E_detach)) + (1-cos(EtoK, E_detach))\n",
    "           + lambda * ScaleReg(K, E, KtoE, EtoK)\n",
    "\n",
    "    Args:\n",
    "        emb_k, emb_e, emb_ktoe, emb_etok: Tensors of shape (batch_size, embedding_dim)\n",
    "        target_norm: The target L2 norm for scale regularization.\n",
    "        lambda_scale: Weight for the scale regularization term.\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss tensor averaged over the batch.\n",
    "    \"\"\"\n",
    "    # --- Anchored Cosine Similarity Loss ---\n",
    "    # Detach emb_e so gradients don't flow back to it from the similarity loss\n",
    "    emb_e_detached = emb_e.detach()\n",
    "\n",
    "    # Calculate similarity only between non-English embeddings and the English anchor\n",
    "    sim_ke = F.cosine_similarity(emb_k, emb_e_detached, dim=1)\n",
    "    sim_ktoee = F.cosine_similarity(emb_ktoe, emb_e_detached, dim=1)\n",
    "    sim_etoke = F.cosine_similarity(emb_etok, emb_e_detached, dim=1)\n",
    "\n",
    "    # Calculate the loss (1 - similarity) for each pair w.r.t the anchor\n",
    "    loss_k_anchor = 1.0 - sim_ke\n",
    "    loss_ktoe_anchor = 1.0 - sim_ktoee\n",
    "    loss_etok_anchor = 1.0 - sim_etoke\n",
    "\n",
    "    # Sum the anchored similarity losses for each item in the batch\n",
    "    # Shape: (batch_size,)\n",
    "    total_anchor_sim_loss = loss_k_anchor + loss_ktoe_anchor + loss_etok_anchor\n",
    "\n",
    "    # Average the similarity loss over the batch\n",
    "    loss_sim_anchor_mean = torch.mean(total_anchor_sim_loss)\n",
    "\n",
    "    # --- Scale Regularization Loss (applied to ALL embeddings) ---\n",
    "    # Use the original emb_e (with gradients enabled) for scale regularization\n",
    "    scale_loss = calculate_scale_regularization(emb_k, emb_e, emb_ktoe, emb_etok, target_norm)\n",
    "    loss_scale_mean = torch.mean(scale_loss)\n",
    "\n",
    "    # --- Total Loss ---\n",
    "    total_loss = loss_sim_anchor_mean + lambda_scale * loss_scale_mean\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingFinetuner:\n",
    "    def __init__(self, model_name=\"BAAI/bge-m3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        \n",
    "        # Save the original model for comparison\n",
    "        self.original_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.original_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    def save_model(self, path=\"finetuned_model\"):\n",
    "        \"\"\"\n",
    "        모델을 저장합니다.\n",
    "        \n",
    "        Args:\n",
    "            path (str): 모델을 저장할 경로/파일명 (확장자 없이)\n",
    "        \"\"\"\n",
    "        # 모델 상태 저장\n",
    "        torch.save(self.model.state_dict(), f\"{path}.pt\")\n",
    "        \n",
    "        # 설정 정보 저장 (선택사항)\n",
    "        import json\n",
    "        config = {\n",
    "            \"model_name\": self.model.config._name_or_path,\n",
    "            \"saved_date\": str(datetime.datetime.now())\n",
    "        }\n",
    "        \n",
    "        with open(f\"{path}_config.json\", \"w\") as f:\n",
    "            json.dump(config, f)\n",
    "        \n",
    "        print(f\"모델이 '{path}.pt'에 저장되었습니다.\")\n",
    "\n",
    "    def load_model(self, path=\"finetuned_model\"):\n",
    "        \"\"\"\n",
    "        저장된 모델을 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            path (str): 불러올 모델 파일 경로 (확장자 없이)\n",
    "        \"\"\"\n",
    "        # 모델 상태 불러오기\n",
    "        self.model.load_state_dict(torch.load(f\"{path}.pt\"))\n",
    "        self.model.eval()  # 평가 모드로 설정\n",
    "        print(f\"모델이 '{path}.pt'에서 불러와졌습니다.\")\n",
    "\n",
    "    def get_embedding(self, model, input_ids, attention_mask):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output\n",
    "    \n",
    "    def finetune(self, train_dataloader, num_epochs=5, lr=1e-5):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch in train_dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Move all inputs to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Get embeddings for all text versions\n",
    "                english_embedding = self.get_embedding(\n",
    "                    self.model, \n",
    "                    batch[\"english_input_ids\"], \n",
    "                    batch[\"english_attention_mask\"]\n",
    "                )\n",
    "                \n",
    "                etok_embedding = self.get_embedding(\n",
    "                    self.model, \n",
    "                    batch[\"etok_input_ids\"], \n",
    "                    batch[\"etok_attention_mask\"]\n",
    "                )\n",
    "                \n",
    "                ktoe_embedding = self.get_embedding(\n",
    "                    self.model, \n",
    "                    batch[\"ktoe_input_ids\"], \n",
    "                    batch[\"ktoe_attention_mask\"]\n",
    "                )\n",
    "                \n",
    "                korean_embedding = self.get_embedding(\n",
    "                    self.model, \n",
    "                    batch[\"korean_input_ids\"], \n",
    "                    batch[\"korean_attention_mask\"]\n",
    "                )\n",
    "                \n",
    "                embeddings = [english_embedding, etok_embedding, ktoe_embedding, korean_embedding]\n",
    "\n",
    "                batch_size = english_embedding.size(0)\n",
    "                num_embeddings = len(embeddings)\n",
    "                num_pairs = num_embeddings * (num_embeddings - 1) // 2\n",
    "                # loss = 0\n",
    "                \n",
    "                # for i in range(num_embeddings):\n",
    "                #     for j in range(i + 1, num_embeddings):\n",
    "                #         # Colculate contrastive loss\n",
    "                #         cos_sim = F.cosine_similarity(embeddings[i], embeddings[j], dim=-1)\n",
    "                #         loss += (1 - cos_sim).mean()\n",
    "                #         # loss += torch.sum(torch.norm(embeddings[i] - embeddings[j], dim=1))\n",
    "                        \n",
    "                #         # Calculate regularization loss\n",
    "                #         total_scale_loss = torch.zeros(english_embedding.shape[0], device=english_embedding.device)\n",
    "\n",
    "                #         for emb in embeddings:\n",
    "                #             norm = torch.norm(emb, p=2, dim=1)\n",
    "                #             scale_loss = (norm - target_norm) ** 2\n",
    "                #             total_scale_loss += scale_loss\n",
    "                \n",
    "                # loss = loss / (num_pairs * batch_size)\n",
    "                \n",
    "                total_loss = compute_cosine_loss_anchor_e_with_scale_reg(\n",
    "                    english_embedding, etok_embedding, ktoe_embedding, korean_embedding,\n",
    "                    target_norm=1.0,  # Example target norm\n",
    "                    lambda_scale=0.5  # Example scale regularization weight\n",
    "                )\n",
    "                loss = total_loss / (num_pairs * batch_size)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_dataloader):.4f}\")\n",
    "    \n",
    "    def compute_embeddings(self, data_loader, model):\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        embeddings = {\n",
    "            \"english\": [],\n",
    "            \"etok\": [],\n",
    "            \"ktoe\": [],\n",
    "            \"korean\": []\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                # Move all inputs to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Compute embeddings for each type\n",
    "                english_emb = self.get_embedding(\n",
    "                    model, \n",
    "                    batch[\"english_input_ids\"], \n",
    "                    batch[\"english_attention_mask\"]\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                etok_emb = self.get_embedding(\n",
    "                    model, \n",
    "                    batch[\"etok_input_ids\"], \n",
    "                    batch[\"etok_attention_mask\"]\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                ktoe_emb = self.get_embedding(\n",
    "                    model, \n",
    "                    batch[\"ktoe_input_ids\"], \n",
    "                    batch[\"ktoe_attention_mask\"]\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                korean_emb = self.get_embedding(\n",
    "                    model, \n",
    "                    batch[\"korean_input_ids\"], \n",
    "                    batch[\"korean_attention_mask\"]\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                # Store embeddings\n",
    "                embeddings[\"english\"].append(english_emb)\n",
    "                embeddings[\"etok\"].append(etok_emb)\n",
    "                embeddings[\"ktoe\"].append(ktoe_emb)\n",
    "                embeddings[\"korean\"].append(korean_emb)\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        for key in embeddings:\n",
    "            if embeddings[key]:  # Check if the list is not empty\n",
    "                embeddings[key] = np.concatenate(embeddings[key], axis=0)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def evaluate_embeddings(self, data_loader):\n",
    "        original_embeddings = self.compute_embeddings(data_loader, self.original_model)\n",
    "        finetuned_embeddings = self.compute_embeddings(data_loader, self.model)\n",
    "        \n",
    "        return original_embeddings, finetuned_embeddings\n",
    "    \n",
    "    def visualize_with_pca(self, original_embeddings, finetuned_embeddings):\n",
    "        # Stack all embeddings for fitting PCA\n",
    "        original_stack = np.vstack([original_embeddings[k] for k in original_embeddings])\n",
    "        finetuned_stack = np.vstack([finetuned_embeddings[k] for k in finetuned_embeddings])\n",
    "        \n",
    "        # Fit PCA on combined data\n",
    "        pca_original = PCA(n_components=2)\n",
    "        pca_finetuned = PCA(n_components=2)\n",
    "        \n",
    "        # Transform each set of embeddings\n",
    "        original_pca_result = {}\n",
    "        finetuned_pca_result = {}\n",
    "        \n",
    "        # Fit PCA on all original embeddings\n",
    "        pca_original.fit(original_stack)\n",
    "        for key in original_embeddings:\n",
    "            original_pca_result[key] = pca_original.transform(original_embeddings[key])\n",
    "        \n",
    "        # Fit PCA on all finetuned embeddings\n",
    "        pca_finetuned.fit(finetuned_stack)\n",
    "        for key in finetuned_embeddings:\n",
    "            finetuned_pca_result[key] = pca_finetuned.transform(finetuned_embeddings[key])\n",
    "        \n",
    "        # Create figure for visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "        \n",
    "        # Define colors and markers for each type\n",
    "        colors = {\n",
    "            \"english\": \"blue\",\n",
    "            \"etok\": \"green\",\n",
    "            \"ktoe\": \"red\",\n",
    "            \"korean\": \"purple\"\n",
    "        }\n",
    "        \n",
    "        markers = {\n",
    "            \"english\": \"o\",\n",
    "            \"etok\": \"s\",\n",
    "            \"ktoe\": \"^\",\n",
    "            \"korean\": \"D\"\n",
    "        }\n",
    "        \n",
    "        # Plot original embeddings\n",
    "        for key in original_pca_result:\n",
    "            ax1.scatter(\n",
    "                original_pca_result[key][:, 0], \n",
    "                original_pca_result[key][:, 1],\n",
    "                color=colors[key],\n",
    "                marker=markers[key],\n",
    "                label=key,\n",
    "                alpha=0.7\n",
    "            )\n",
    "        \n",
    "        ax1.set_title(\"Original Model Embeddings (Before Fine-tuning)\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot finetuned embeddings\n",
    "        for key in finetuned_pca_result:\n",
    "            ax2.scatter(\n",
    "                finetuned_pca_result[key][:, 0], \n",
    "                finetuned_pca_result[key][:, 1],\n",
    "                color=colors[key],\n",
    "                marker=markers[key],\n",
    "                label=key,\n",
    "                alpha=0.7\n",
    "            )\n",
    "        \n",
    "        ax2.set_title(\"Fine-tuned Model Embeddings\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.suptitle(\"Comparing Embeddings Before and After Fine-tuning\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"embedding_comparison.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate and report average distances\n",
    "        print(\"\\nAverage L2 distances from English embeddings:\")\n",
    "        print(\"Before fine-tuning:\")\n",
    "        for key in [\"etok\", \"ktoe\", \"korean\"]:\n",
    "            avg_dist = np.mean(np.linalg.norm(\n",
    "                original_embeddings[\"english\"] - original_embeddings[key], axis=1\n",
    "            ))\n",
    "            print(f\"  English to {key}: {avg_dist:.4f}\")\n",
    "        \n",
    "        print(\"\\nAfter fine-tuning:\")\n",
    "        for key in [\"etok\", \"ktoe\", \"korean\"]:\n",
    "            avg_dist = np.mean(np.linalg.norm(\n",
    "                finetuned_embeddings[\"english\"] - finetuned_embeddings[key], axis=1\n",
    "            ))\n",
    "            print(f\"  English to {key}: {avg_dist:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train data: 215\n",
      "# Validation data: 54\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "file_path = \"code-switch.json\"\n",
    "file_path1 = \"code-switch1.json\"\n",
    "file_path2 = \"code-switch2.json\"\n",
    "file_paths = [file_path, file_path1, file_path2]\n",
    "\n",
    "# Load the JSON data\n",
    "data = []\n",
    "for fp in file_paths:\n",
    "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "        data += json.load(f)\n",
    "\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)\n",
    "# with open(file_path1, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data1 = json.load(f)\n",
    "# with open(file_path2, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data2 = json.load(f)\n",
    "\n",
    "# # Combine the data\n",
    "# data = data + data1 + data2\n",
    "\n",
    "# Shuffle the data for randomness\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data into 80% training and 20% validation\n",
    "train_size = int(0.8 * len(data))\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]\n",
    "print(\"# Train data:\", len(train_data))\n",
    "print(\"# Validation data:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the finetuner\n",
    "finetuner = EmbeddingFinetuner()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MultilingualDataset(data, finetuner.tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "할당된 GPU 메모리 (Allocated): 15.45 GB\n",
      "예약된 GPU 메모리 (Reserved): 15.52 GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    # model.to(device) # 모델을 GPU로 이동\n",
    "\n",
    "    # 현재 활성 텐서들이 사용 중인 GPU 메모리 (Bytes)\n",
    "    allocated_memory = torch.cuda.memory_allocated(device=device)\n",
    "    # PyTorch 캐싱 할당자가 예약한 총 GPU 메모리 (Bytes)\n",
    "    reserved_memory = torch.cuda.memory_reserved(device=device)\n",
    "\n",
    "    print(f\"할당된 GPU 메모리 (Allocated): {allocated_memory / (1024**3):.2f} GB\")\n",
    "    print(f\"예약된 GPU 메모리 (Reserved): {reserved_memory / (1024**3):.2f} GB\")\n",
    "\n",
    "    # 더 상세한 메모리 사용 현황 출력\n",
    "    # print(torch.cuda.memory_summary(device=device, abbreviated=False))\n",
    "else:\n",
    "    print(\"CUDA (GPU)를 사용할 수 없습니다.\")\n",
    "    # CPU 메모리 사용량은 psutil 같은 외부 라이브러리로 확인해야 합니다.\n",
    "    # import psutil\n",
    "    # process = psutil.Process(os.getpid())\n",
    "    # print(f\"현재 프로세스 RAM 사용량: {process.memory_info().rss / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16679"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.72 GiB of which 3.44 MiB is free. Process 580622 has 15.71 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 69.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Finetune the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mfinetuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m finetuner\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilingual_embedding_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Evaluate embeddings\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 121\u001b[0m, in \u001b[0;36mEmbeddingFinetuner.finetune\u001b[0;34m(self, train_dataloader, num_epochs, lr)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 121\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/bgem3-finetune/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/bgem3-finetune/.venv/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/bgem3-finetune/.venv/lib/python3.10/site-packages/torch/optim/adam.py:234\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    231\u001b[0m     state_steps: List[Tensor] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m     adam(\n\u001b[1;32m    245\u001b[0m         params_with_grad,\n\u001b[1;32m    246\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    265\u001b[0m     )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/bgem3-finetune/.venv/lib/python3.10/site-packages/torch/optim/adam.py:174\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    164\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    165\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    166\u001b[0m         (),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m_get_scalar_dtype())\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[0;32m--> 174\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Exponential moving average of squared gradient values\u001b[39;00m\n\u001b[1;32m    178\u001b[0m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_avg_sq\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\n\u001b[1;32m    179\u001b[0m     p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format\n\u001b[1;32m    180\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 15.72 GiB of which 3.44 MiB is free. Process 580622 has 15.71 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 69.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Initialize the finetuner\n",
    "finetuner = EmbeddingFinetuner()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MultilingualDataset(data, finetuner.tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Finetune the model\n",
    "finetuner.finetune(dataloader, num_epochs=10, lr=2e-5)\n",
    "\n",
    "finetuner.save_model(\"multilingual_embedding_model\")\n",
    "\n",
    "# Evaluate embeddings\n",
    "original_embeddings, finetuned_embeddings = finetuner.evaluate_embeddings(dataloader)\n",
    "\n",
    "# Visualize results\n",
    "finetuner.visualize_with_pca(original_embeddings, finetuned_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import pool\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.models import Transformer, Pooling\n",
    "\n",
    "# 1. 원래 모델과 토크나이저 로드\n",
    "model = torch.load(\"multilingual_embedding_model.pt\", map_location=torch.device('cpu'))  # 로컬 .pt 파일\n",
    "# tokenizer = finetuner.tokenizer\n",
    "\n",
    "# # 2. 모델을 SentenceTransformer로 래핑\n",
    "# # modules는 Transformer와 Pooling 레이어로 구성\n",
    "transformer = Transformer(model_name_or_path=\"BAAI/bge-m3\", max_seq_length=128)\n",
    "transformer.auto_model = finetuner.model  # 로드한 모델로 교체\n",
    "pooling = Pooling(finetuner.model.config.hidden_size, pooling_mode=\"mean\")  # bge-m3은 mean pooling 사용\n",
    "\n",
    "st_model = SentenceTransformer(modules=[transformer, pooling])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 변환된 모델 저장\n",
    "st_model.save(\"bge_m3_finetuned_local\")\n",
    "print(\"모델이 'bge_m3_finetuned_local'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import get_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=get_secret(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e0bf691793404b971124e125ab8409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `save_to_hub` method is deprecated and will be removed in a future version of SentenceTransformers. Please use `push_to_hub` instead for future model uploads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 업로드 중 오류 발생: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-680666c8-24b07e7b0025856323912270;f6712060-d536-4041-b43d-3c6b0e81b2a5)\n",
      "\n",
      "Invalid username or password.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    repo_id = \"hyoseok1989/bge-m3-finetuned\"\n",
    "    commit_msg = \"Fine-tuned BGE-M3 for Korean-English code-switching task\"\n",
    "\n",
    "    st_model.save_to_hub(\n",
    "        repo_id=repo_id,\n",
    "        commit_message=commit_msg,\n",
    "        private=False,\n",
    "        exist_ok=True,  # True if overwrite the existing model\n",
    "    )\n",
    "    print(f\"모델이 Hugging Face Hub에 성공적으로 업로드되었습니다: {repo_id}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"모델 업로드 중 오류 발생: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
