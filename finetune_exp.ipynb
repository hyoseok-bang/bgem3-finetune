{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilingualDataset(Dataset):\n",
    "    def __init__(self, data_list, tokenizer, max_length=128):\n",
    "        self.data = data_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Tokenize all versions of the sentence\n",
    "        english = self.tokenizer(item[\"English\"], padding=\"max_length\", \n",
    "                                truncation=True, max_length=self.max_length, \n",
    "                                return_tensors=\"pt\")\n",
    "        \n",
    "        etok = self.tokenizer(item[\"EtoK\"], padding=\"max_length\", \n",
    "                             truncation=True, max_length=self.max_length, \n",
    "                             return_tensors=\"pt\")\n",
    "        \n",
    "        ktoe = self.tokenizer(item[\"KtoE\"], padding=\"max_length\", \n",
    "                             truncation=True, max_length=self.max_length, \n",
    "                             return_tensors=\"pt\")\n",
    "        \n",
    "        korean = self.tokenizer(item[\"Korean\"], padding=\"max_length\", \n",
    "                               truncation=True, max_length=self.max_length, \n",
    "                               return_tensors=\"pt\")\n",
    "        \n",
    "        # Return a flat dictionary with prefixed keys\n",
    "        return {\n",
    "            \"english_input_ids\": english[\"input_ids\"].squeeze(0),\n",
    "            \"english_attention_mask\": english[\"attention_mask\"].squeeze(0),\n",
    "            \"etok_input_ids\": etok[\"input_ids\"].squeeze(0),\n",
    "            \"etok_attention_mask\": etok[\"attention_mask\"].squeeze(0),\n",
    "            \"ktoe_input_ids\": ktoe[\"input_ids\"].squeeze(0),\n",
    "            \"ktoe_attention_mask\": ktoe[\"attention_mask\"].squeeze(0),\n",
    "            \"korean_input_ids\": korean[\"input_ids\"].squeeze(0),\n",
    "            \"korean_attention_mask\": korean[\"attention_mask\"].squeeze(0)\n",
    "        }\n",
    "\n",
    "\n",
    "# --- Helper Function to Calculate Pairwise Loss ---\n",
    "def calculate_pairwise_loss(\n",
    "    emb_k: torch.Tensor,\n",
    "    emb_e: torch.Tensor,\n",
    "    emb_ktoe: torch.Tensor,\n",
    "    emb_etok: torch.Tensor,\n",
    "    loss_type: str = 'cosine' # 'cosine' or 'mse'\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the sum of pairwise losses (1-cosine_sim or mse)\n",
    "    between all unique pairs in the quadruplet.\n",
    "\n",
    "    Args:\n",
    "        emb_k, emb_e, emb_ktoe, emb_etok: Tensors of shape (batch_size, embedding_dim)\n",
    "        loss_type: 'cosine' for 1 - cosine_similarity, 'mse' for squared L2 distance.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size,) containing the sum of pairwise losses for each item.\n",
    "    \"\"\"\n",
    "    embeddings = [emb_k, emb_e, emb_ktoe, emb_etok]\n",
    "    batch_size = emb_k.shape[0]\n",
    "    total_pairwise_loss = torch.zeros(batch_size, device=emb_k.device)\n",
    "    num_pairs = 0\n",
    "\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i + 1, len(embeddings)):\n",
    "            emb_i = embeddings[i]\n",
    "            emb_j = embeddings[j]\n",
    "\n",
    "            if loss_type == 'cosine':\n",
    "                # 1 - cosine similarity\n",
    "                sim = F.cosine_similarity(emb_i, emb_j, dim=1)\n",
    "                loss = 1.0 - sim\n",
    "            elif loss_type == 'mse':\n",
    "                # Squared L2 distance\n",
    "                loss = torch.sum((emb_i - emb_j) ** 2, dim=1)\n",
    "            else:\n",
    "                raise ValueError(\"loss_type must be 'cosine' or 'mse'\")\n",
    "\n",
    "            total_pairwise_loss += loss\n",
    "            num_pairs += 1\n",
    "\n",
    "    # Optional: Average over the number of pairs if desired,\n",
    "    # but summing works fine as it's consistent.\n",
    "    # return total_pairwise_loss / num_pairs\n",
    "    return total_pairwise_loss\n",
    "\n",
    "# --- Helper Function for Scale Regularization ---\n",
    "def calculate_scale_regularization(\n",
    "    emb_k: torch.Tensor,\n",
    "    emb_e: torch.Tensor,\n",
    "    emb_ktoe: torch.Tensor,\n",
    "    emb_etok: torch.Tensor,\n",
    "    target_norm: float\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the scale regularization loss.\n",
    "\n",
    "    Args:\n",
    "        emb_k, emb_e, emb_ktoe, emb_etok: Tensors of shape (batch_size, embedding_dim)\n",
    "        target_norm: The target L2 norm scalar value.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (batch_size,) containing the scale regularization loss for each item.\n",
    "    \"\"\"\n",
    "    embeddings = [emb_k, emb_e, emb_ktoe, emb_etok]\n",
    "    total_scale_loss = torch.zeros(emb_k.shape[0], device=emb_k.device)\n",
    "\n",
    "    for emb in embeddings:\n",
    "        norm = torch.norm(emb, p=2, dim=1)\n",
    "        scale_loss = (norm - target_norm) ** 2\n",
    "        total_scale_loss += scale_loss\n",
    "\n",
    "    # Average over the 4 embedding types\n",
    "    return total_scale_loss / len(embeddings)\n",
    "\n",
    "\n",
    "# --- Loss Function Implementations ---\n",
    "\n",
    "def compute_cosine_loss_with_scale_reg(\n",
    "    emb_k: torch.Tensor,\n",
    "    emb_e: torch.Tensor,\n",
    "    emb_ktoe: torch.Tensor,\n",
    "    emb_etok: torch.Tensor,\n",
    "    target_norm: float,\n",
    "    lambda_scale: float = 0.01 # Hyperparameter to balance terms\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes Loss = (Pairwise 1-CosineSim) + lambda * (Scale Regularization)\n",
    "\n",
    "    Args:\n",
    "        emb_k, emb_e, emb_ktoe, emb_etok: Tensors of shape (batch_size, embedding_dim)\n",
    "        target_norm: The target L2 norm.\n",
    "        lambda_scale: Weight for the scale regularization term.\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss tensor averaged over the batch.\n",
    "    \"\"\"\n",
    "    # Pairwise Cosine Similarity Loss (mean over batch)\n",
    "    pairwise_loss = calculate_pairwise_loss(emb_k, emb_e, emb_ktoe, emb_etok, loss_type='cosine')\n",
    "    loss_sim = torch.mean(pairwise_loss)\n",
    "\n",
    "    # Scale Regularization Loss (mean over batch)\n",
    "    scale_loss = calculate_scale_regularization(emb_k, emb_e, emb_ktoe, emb_etok, target_norm)\n",
    "    loss_scale = torch.mean(scale_loss)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_sim + lambda_scale * loss_scale\n",
    "    return total_loss\n",
    "\n",
    "def compute_mse_loss_with_scale_reg(\n",
    "    emb_k: torch.Tensor,\n",
    "    emb_e: torch.Tensor,\n",
    "    emb_ktoe: torch.Tensor,\n",
    "    emb_etok: torch.Tensor,\n",
    "    target_norm: float,\n",
    "    lambda_scale: float = 0.01 # Hyperparameter to balance terms\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes Loss = (Pairwise MSE) + lambda * (Scale Regularization)\n",
    "\n",
    "    Args:\n",
    "        emb_k, emb_e, emb_ktoe, emb_etok: Tensors of shape (batch_size, embedding_dim)\n",
    "        target_norm: The target L2 norm.\n",
    "        lambda_scale: Weight for the scale regularization term.\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss tensor averaged over the batch.\n",
    "    \"\"\"\n",
    "    # Pairwise MSE Loss (mean over batch)\n",
    "    pairwise_loss = calculate_pairwise_loss(emb_k, emb_e, emb_ktoe, emb_etok, loss_type='mse')\n",
    "    loss_dist = torch.mean(pairwise_loss)\n",
    "\n",
    "    # Scale Regularization Loss (mean over batch)\n",
    "    scale_loss = calculate_scale_regularization(emb_k, emb_e, emb_ktoe, emb_etok, target_norm)\n",
    "    loss_scale = torch.mean(scale_loss)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_dist + lambda_scale * loss_scale\n",
    "    return total_loss\n",
    "\n",
    "# --- Placeholder for Scale-Regularized Contrastive Loss (InfoNCE style) ---\n",
    "# This requires a more complex setup with negative sampling, often done via in-batch negatives.\n",
    "\n",
    "def compute_infonce_loss_with_scale_reg(\n",
    "    emb_k: torch.Tensor,\n",
    "    emb_e: torch.Tensor,\n",
    "    emb_ktoe: torch.Tensor,\n",
    "    emb_etok: torch.Tensor,\n",
    "    target_norm: float,\n",
    "    lambda_scale: float = 0.01,\n",
    "    temperature: float = 0.07\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes InfoNCE-style Contrastive Loss + Scale Regularization.\n",
    "    Assumes in-batch negative sampling.\n",
    "\n",
    "    Args:\n",
    "        emb_k, emb_e, emb_ktoe, emb_etok: Tensors of shape (batch_size, embedding_dim)\n",
    "        target_norm: The target L2 norm.\n",
    "        lambda_scale: Weight for the scale regularization term.\n",
    "        temperature: Temperature scaling factor for InfoNCE.\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss tensor averaged over the batch.\n",
    "    \"\"\"\n",
    "    device = emb_k.device\n",
    "    batch_size = emb_k.shape[0]\n",
    "    embedding_dim = emb_k.shape[1]\n",
    "\n",
    "    # Concatenate all embeddings for easier similarity matrix calculation\n",
    "    # Shape: (batch_size * 4, embedding_dim)\n",
    "    all_embeddings = torch.cat([emb_k, emb_e, emb_ktoe, emb_etok], dim=0)\n",
    "\n",
    "    # Calculate pairwise cosine similarities (logits)\n",
    "    # Shape: (batch_size * 4, batch_size * 4)\n",
    "    logits = F.cosine_similarity(all_embeddings.unsqueeze(1), all_embeddings.unsqueeze(0), dim=2)\n",
    "    logits /= temperature # Apply temperature scaling\n",
    "\n",
    "    # Create labels and masks for InfoNCE\n",
    "    # Labels indicate the index of the positive counterpart within the large batch.\n",
    "    # For InfoNCE, typically anchor vs one positive, others negative.\n",
    "    # Here, we have multiple positives per anchor within the quadruplet.\n",
    "\n",
    "    # Simplified approach: Treat each K as anchor, E/KtoE/EtoK as positives. Repeat for other anchors.\n",
    "    total_contrastive_loss = torch.tensor(0.0, device=device)\n",
    "    anchors = [emb_k, emb_e, emb_ktoe, emb_etok]\n",
    "    anchor_indices = [torch.arange(batch_size) + i * batch_size for i in range(4)] # Indices for K, E, KtoE, EtoK in all_embeddings\n",
    "\n",
    "    for i in range(4): # Iterate through K, E, KtoE, EtoK as anchors\n",
    "        current_anchor_indices = anchor_indices[i]\n",
    "        # Logits for the current anchors vs all embeddings\n",
    "        anchor_logits = logits[current_anchor_indices] # Shape: (batch_size, batch_size * 4)\n",
    "\n",
    "        # Create target labels: indicate positive pairs\n",
    "        # Positive keys are the other embeddings from the *same* original item.\n",
    "        labels = torch.zeros_like(anchor_logits, dtype=torch.bool)\n",
    "        for j in range(4):\n",
    "            if i == j: continue # Don't mark anchor as its own positive\n",
    "            positive_indices = anchor_indices[j]\n",
    "            # Mark True where the column index corresponds to a positive pair for the row anchor\n",
    "            # Need element-wise check. A bit tricky with broadcasted indices.\n",
    "            # Let's usearange for batch items:\n",
    "            batch_indices = torch.arange(batch_size, device=device)\n",
    "            labels[batch_indices, positive_indices] = True\n",
    "\n",
    "        # InfoNCE Loss calculation: log_softmax over all pairs, gather positives.\n",
    "        log_prob = F.log_softmax(anchor_logits, dim=1)\n",
    "\n",
    "        # Sum log probabilities of *all* positive pairs for the anchor\n",
    "        # Avoid division by zero if no positives (shouldn't happen here)\n",
    "        mean_log_prob_pos = (log_prob * labels).sum(1) / labels.sum(1).clamp(min=1)\n",
    "\n",
    "        # Contrastive loss is the negative of the mean log probability of positives\n",
    "        contrastive_loss = -mean_log_prob_pos\n",
    "        total_contrastive_loss += contrastive_loss.mean() # Average over batch\n",
    "\n",
    "    # Average contrastive loss over the 4 anchor types\n",
    "    loss_contrastive = total_contrastive_loss / 4.0\n",
    "\n",
    "    # Scale Regularization Loss (mean over batch)\n",
    "    scale_loss = calculate_scale_regularization(emb_k, emb_e, emb_ktoe, emb_etok, target_norm)\n",
    "    loss_scale = torch.mean(scale_loss)\n",
    "\n",
    "    # Total Loss\n",
    "    total_loss = loss_contrastive + lambda_scale * loss_scale\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingFinetuner:\n",
    "    def __init__(self, model_name=\"BAAI/bge-m3\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        \n",
    "        # Save the original model for comparison\n",
    "        self.original_model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.original_model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    def save_model(self, path=\"finetuned_model\"):\n",
    "        \"\"\"\n",
    "        모델을 저장합니다.\n",
    "        \n",
    "        Args:\n",
    "            path (str): 모델을 저장할 경로/파일명 (확장자 없이)\n",
    "        \"\"\"\n",
    "        # 모델 상태 저장\n",
    "        torch.save(self.model.state_dict(), f\"{path}.pt\")\n",
    "        \n",
    "        # 설정 정보 저장 (선택사항)\n",
    "        import json\n",
    "        config = {\n",
    "            \"model_name\": self.model.config._name_or_path,\n",
    "            \"saved_date\": str(datetime.datetime.now())\n",
    "        }\n",
    "        \n",
    "        with open(f\"{path}_config.json\", \"w\") as f:\n",
    "            json.dump(config, f)\n",
    "        \n",
    "        print(f\"모델이 '{path}.pt'에 저장되었습니다.\")\n",
    "\n",
    "    def load_model(self, path=\"finetuned_model\"):\n",
    "        \"\"\"\n",
    "        저장된 모델을 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            path (str): 불러올 모델 파일 경로 (확장자 없이)\n",
    "        \"\"\"\n",
    "        # 모델 상태 불러오기\n",
    "        self.model.load_state_dict(torch.load(f\"{path}.pt\"))\n",
    "        self.model.eval()  # 평가 모드로 설정\n",
    "        print(f\"모델이 '{path}.pt'에서 불러와졌습니다.\")\n",
    "\n",
    "    def get_embedding(self, model, input_ids, attention_mask):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output\n",
    "    \n",
    "    def finetune(self, train_dataloader, num_epochs=5, lr=1e-5):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch in train_dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Move all inputs to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Get embeddings for all text versions\n",
    "                english_embedding = self.get_embedding(\n",
    "                    self.model, \n",
    "                    batch[\"english_input_ids\"], \n",
    "                    batch[\"english_attention_mask\"]\n",
    "                )\n",
    "                \n",
    "                etok_embedding = self.get_embedding(\n",
    "                    self.model, \n",
    "                    batch[\"etok_input_ids\"], \n",
    "                    batch[\"etok_attention_mask\"]\n",
    "                )\n",
    "                \n",
    "                ktoe_embedding = self.get_embedding(\n",
    "                    self.model, \n",
    "                    batch[\"ktoe_input_ids\"], \n",
    "                    batch[\"ktoe_attention_mask\"]\n",
    "                )\n",
    "                \n",
    "                korean_embedding = self.get_embedding(\n",
    "                    self.model, \n",
    "                    batch[\"korean_input_ids\"], \n",
    "                    batch[\"korean_attention_mask\"]\n",
    "                )\n",
    "                \n",
    "                embeddings = [english_embedding, etok_embedding, ktoe_embedding, korean_embedding]\n",
    "\n",
    "                batch_size = english_embedding.size(0)\n",
    "                num_embeddings = len(embeddings)\n",
    "                num_pairs = num_embeddings * (num_embeddings - 1) // 2\n",
    "                # loss = 0\n",
    "                \n",
    "                # for i in range(num_embeddings):\n",
    "                #     for j in range(i + 1, num_embeddings):\n",
    "                #         # Colculate contrastive loss\n",
    "                #         cos_sim = F.cosine_similarity(embeddings[i], embeddings[j], dim=-1)\n",
    "                #         loss += (1 - cos_sim).mean()\n",
    "                #         # loss += torch.sum(torch.norm(embeddings[i] - embeddings[j], dim=1))\n",
    "                        \n",
    "                #         # Calculate regularization loss\n",
    "                #         total_scale_loss = torch.zeros(english_embedding.shape[0], device=english_embedding.device)\n",
    "\n",
    "                #         for emb in embeddings:\n",
    "                #             norm = torch.norm(emb, p=2, dim=1)\n",
    "                #             scale_loss = (norm - target_norm) ** 2\n",
    "                #             total_scale_loss += scale_loss\n",
    "                \n",
    "                # loss = loss / (num_pairs * batch_size)\n",
    "                \n",
    "                total_loss = compute_cosine_loss_with_scale_reg(\n",
    "                    english_embedding, etok_embedding, ktoe_embedding, korean_embedding,\n",
    "                    target_norm=1.0,  # Example target norm\n",
    "                    lambda_scale=0.1  # Example scale regularization weight\n",
    "                )\n",
    "                loss = total_loss / (num_pairs * batch_size)\n",
    "                \n",
    "                # Backward pass and optimization\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_dataloader):.4f}\")\n",
    "    \n",
    "    def compute_embeddings(self, data_loader, model):\n",
    "        # Set model to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        embeddings = {\n",
    "            \"english\": [],\n",
    "            \"etok\": [],\n",
    "            \"ktoe\": [],\n",
    "            \"korean\": []\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in data_loader:\n",
    "                # Move all inputs to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Compute embeddings for each type\n",
    "                english_emb = self.get_embedding(\n",
    "                    model, \n",
    "                    batch[\"english_input_ids\"], \n",
    "                    batch[\"english_attention_mask\"]\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                etok_emb = self.get_embedding(\n",
    "                    model, \n",
    "                    batch[\"etok_input_ids\"], \n",
    "                    batch[\"etok_attention_mask\"]\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                ktoe_emb = self.get_embedding(\n",
    "                    model, \n",
    "                    batch[\"ktoe_input_ids\"], \n",
    "                    batch[\"ktoe_attention_mask\"]\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                korean_emb = self.get_embedding(\n",
    "                    model, \n",
    "                    batch[\"korean_input_ids\"], \n",
    "                    batch[\"korean_attention_mask\"]\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                # Store embeddings\n",
    "                embeddings[\"english\"].append(english_emb)\n",
    "                embeddings[\"etok\"].append(etok_emb)\n",
    "                embeddings[\"ktoe\"].append(ktoe_emb)\n",
    "                embeddings[\"korean\"].append(korean_emb)\n",
    "        \n",
    "        # Concatenate all batches\n",
    "        for key in embeddings:\n",
    "            if embeddings[key]:  # Check if the list is not empty\n",
    "                embeddings[key] = np.concatenate(embeddings[key], axis=0)\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def evaluate_embeddings(self, data_loader):\n",
    "        original_embeddings = self.compute_embeddings(data_loader, self.original_model)\n",
    "        finetuned_embeddings = self.compute_embeddings(data_loader, self.model)\n",
    "        \n",
    "        return original_embeddings, finetuned_embeddings\n",
    "    \n",
    "    def visualize_with_pca(self, original_embeddings, finetuned_embeddings):\n",
    "        # Stack all embeddings for fitting PCA\n",
    "        original_stack = np.vstack([original_embeddings[k] for k in original_embeddings])\n",
    "        finetuned_stack = np.vstack([finetuned_embeddings[k] for k in finetuned_embeddings])\n",
    "        \n",
    "        # Fit PCA on combined data\n",
    "        pca_original = PCA(n_components=2)\n",
    "        pca_finetuned = PCA(n_components=2)\n",
    "        \n",
    "        # Transform each set of embeddings\n",
    "        original_pca_result = {}\n",
    "        finetuned_pca_result = {}\n",
    "        \n",
    "        # Fit PCA on all original embeddings\n",
    "        pca_original.fit(original_stack)\n",
    "        for key in original_embeddings:\n",
    "            original_pca_result[key] = pca_original.transform(original_embeddings[key])\n",
    "        \n",
    "        # Fit PCA on all finetuned embeddings\n",
    "        pca_finetuned.fit(finetuned_stack)\n",
    "        for key in finetuned_embeddings:\n",
    "            finetuned_pca_result[key] = pca_finetuned.transform(finetuned_embeddings[key])\n",
    "        \n",
    "        # Create figure for visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "        \n",
    "        # Define colors and markers for each type\n",
    "        colors = {\n",
    "            \"english\": \"blue\",\n",
    "            \"etok\": \"green\",\n",
    "            \"ktoe\": \"red\",\n",
    "            \"korean\": \"purple\"\n",
    "        }\n",
    "        \n",
    "        markers = {\n",
    "            \"english\": \"o\",\n",
    "            \"etok\": \"s\",\n",
    "            \"ktoe\": \"^\",\n",
    "            \"korean\": \"D\"\n",
    "        }\n",
    "        \n",
    "        # Plot original embeddings\n",
    "        for key in original_pca_result:\n",
    "            ax1.scatter(\n",
    "                original_pca_result[key][:, 0], \n",
    "                original_pca_result[key][:, 1],\n",
    "                color=colors[key],\n",
    "                marker=markers[key],\n",
    "                label=key,\n",
    "                alpha=0.7\n",
    "            )\n",
    "        \n",
    "        ax1.set_title(\"Original Model Embeddings (Before Fine-tuning)\")\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Plot finetuned embeddings\n",
    "        for key in finetuned_pca_result:\n",
    "            ax2.scatter(\n",
    "                finetuned_pca_result[key][:, 0], \n",
    "                finetuned_pca_result[key][:, 1],\n",
    "                color=colors[key],\n",
    "                marker=markers[key],\n",
    "                label=key,\n",
    "                alpha=0.7\n",
    "            )\n",
    "        \n",
    "        ax2.set_title(\"Fine-tuned Model Embeddings\")\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.suptitle(\"Comparing Embeddings Before and After Fine-tuning\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"embedding_comparison.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate and report average distances\n",
    "        print(\"\\nAverage L2 distances from English embeddings:\")\n",
    "        print(\"Before fine-tuning:\")\n",
    "        for key in [\"etok\", \"ktoe\", \"korean\"]:\n",
    "            avg_dist = np.mean(np.linalg.norm(\n",
    "                original_embeddings[\"english\"] - original_embeddings[key], axis=1\n",
    "            ))\n",
    "            print(f\"  English to {key}: {avg_dist:.4f}\")\n",
    "        \n",
    "        print(\"\\nAfter fine-tuning:\")\n",
    "        for key in [\"etok\", \"ktoe\", \"korean\"]:\n",
    "            avg_dist = np.mean(np.linalg.norm(\n",
    "                finetuned_embeddings[\"english\"] - finetuned_embeddings[key], axis=1\n",
    "            ))\n",
    "            print(f\"  English to {key}: {avg_dist:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train data: 80\n",
      "# Validation data: 21\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "file_path = \"code-switch.json\"\n",
    "file_path1 = \"code-switch1.json\"\n",
    "file_path2 = \"code-switch2.json\"\n",
    "# file_paths = [file_path, file_path1, file_path2]\n",
    "file_paths = [file_path]\n",
    "\n",
    "# Load the JSON data\n",
    "data = []\n",
    "for fp in file_paths:\n",
    "    with open(fp, \"r\", encoding=\"utf-8\") as f:\n",
    "        data += json.load(f)\n",
    "\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)\n",
    "# with open(file_path1, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data1 = json.load(f)\n",
    "# with open(file_path2, \"r\", encoding=\"utf-8\") as f:\n",
    "#     data2 = json.load(f)\n",
    "\n",
    "# # Combine the data\n",
    "# data = data + data1 + data2\n",
    "\n",
    "# Shuffle the data for randomness\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Split the data into 80% training and 20% validation\n",
    "train_size = int(0.8 * len(data))\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:]\n",
    "print(\"# Train data:\", len(train_data))\n",
    "print(\"# Validation data:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the finetuner\n",
    "finetuner = EmbeddingFinetuner()\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = MultilingualDataset(data, finetuner.tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = list(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embedding = finetuner.get_embedding(finetuner.model, input_ids=sample_data[0][\"english_input_ids\"], attention_mask=sample_data[0][\"english_attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     40\u001b[39m     finetuner.visualize_with_pca(original_embeddings, finetuned_embeddings)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     29\u001b[39m dataloader = DataLoader(dataset, batch_size=\u001b[32m1\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Finetune the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mfinetuner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m finetuner.save_model(\u001b[33m\"\u001b[39m\u001b[33mmultilingual_embedding_model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Evaluate embeddings\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 108\u001b[39m, in \u001b[36mEmbeddingFinetuner.finetune\u001b[39m\u001b[34m(self, train_dataloader, num_epochs, lr)\u001b[39m\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n\u001b[32m    107\u001b[39m     loss.backward()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     total_loss += loss.item()\n\u001b[32m    112\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss/\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/torch/optim/adam.py:244\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    232\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    234\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    235\u001b[39m         group,\n\u001b[32m    236\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m         state_steps,\n\u001b[32m    242\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/torch/optim/adam.py:876\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    874\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/bge-m3/lib/python3.12/site-packages/torch/optim/adam.py:423\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    420\u001b[39m     device_beta1 = beta1\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m \u001b[43mexp_avg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_beta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    425\u001b[39m exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=\u001b[32m1\u001b[39m - beta2)\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # # Example data with multiple samples for training\n",
    "    # data = [\n",
    "    #     {\n",
    "    #         \"EtoK\": \"Could you explain how 나노 로봇 and 자기장 조작 can deliver drugs to specific cells?\",\n",
    "    #         \"KtoE\": \"Nanorobotics와 magnetic control을 활용해 특정 세포에 약물을 운반하는 원리는 무엇인가요?\",\n",
    "    #         \"English\": \"Could you explain how nanorobotics and magnetic control can deliver drugs to specific cells?\",\n",
    "    #         \"Korean\": \"나노 로봇과 자기장 조작을 통해 특정 세포로 약물을 전달하는 원리는 무엇인가요?\"\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"EtoK\": \"The 인공지능 system can 학습 from large datasets.\",\n",
    "    #         \"KtoE\": \"artificial intelligence 시스템은 대규모 데이터셋에서 learn 할 수 있습니다.\",\n",
    "    #         \"English\": \"The artificial intelligence system can learn from large datasets.\",\n",
    "    #         \"Korean\": \"인공지능 시스템은 대규모 데이터셋에서 학습할 수 있습니다.\"\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"EtoK\": \"Can 양자 컴퓨팅 solve problems that 고전적 컴퓨터 cannot?\",\n",
    "    #         \"KtoE\": \"Quantum computing이 classical computers가 해결할 수 없는 문제를 해결할 수 있을까요?\",\n",
    "    #         \"English\": \"Can quantum computing solve problems that classical computers cannot?\",\n",
    "    #         \"Korean\": \"양자 컴퓨팅이 고전적 컴퓨터가 해결할 수 없는 문제를 해결할 수 있을까요?\"\n",
    "    #     }\n",
    "    # ]\n",
    "    \n",
    "    # Initialize the finetuner\n",
    "    finetuner = EmbeddingFinetuner()\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = MultilingualDataset(data, finetuner.tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # Finetune the model\n",
    "    finetuner.finetune(dataloader, num_epochs=10, lr=2e-5)\n",
    "    \n",
    "    finetuner.save_model(\"multilingual_embedding_model\")\n",
    "    \n",
    "    # Evaluate embeddings\n",
    "    original_embeddings, finetuned_embeddings = finetuner.evaluate_embeddings(dataloader)\n",
    "    \n",
    "    # Visualize results\n",
    "    finetuner.visualize_with_pca(original_embeddings, finetuned_embeddings)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bge-m3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
